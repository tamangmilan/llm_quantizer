{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install transformers\n%pip install -U \"huggingface_hub[cli]\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom transformers import OPTForCausalLM, AutoTokenizer, AutoConfig\n\nfrom huggingface_hub import HfApi, create_repo\nfrom huggingface_hub import hf_hub_download\n\n# make sure to use your own auth token as I'll be deleting it after publish this post.\n!huggingface-cli login --token hf_THkbLhyIHHmluGkwwnzpXOvR########## \ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T09:20:17.886969Z","iopub.execute_input":"2024-06-29T09:20:17.887273Z","iopub.status.idle":"2024-06-29T09:20:40.193760Z","shell.execute_reply.started":"2024-06-29T09:20:17.887246Z","shell.execute_reply":"2024-06-29T09:20:40.192757Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-29 09:20:25.786831: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-29 09:20:25.786951: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-29 09:20:25.951224: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define quantizer class QuantizedLinearLayer and its functions quantize and foward function\nclass QuantizedLinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias = True, dtype=torch.float32):\n        super().__init__()\n        \n        self.register_buffer(\"weight\", torch.randint(-8, 7, (out_features, in_features)).to(torch.int8).to(device))\n        self.register_buffer(\"scale\", torch.randn((out_features), dtype=dtype).to(device))\n\n        if bias:\n            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype).to(device))\n        else:\n            self.bias = None\n\n# For 4-bit quantization\n    def quantize(self, weight):\n        weight_f32 = weight.clone().to(torch.float32).to(device)\n        scale = weight_f32.abs().max(dim=-1).values/7\n        scale = scale.to(weight.dtype)\n\n        quantized_weight = torch.clamp(torch.round(weight/scale.unsqueeze(1)), -8, 7).to(torch.int8).to(device)    \n        # Further quantized the weight to 4-bit by PACKING THE WEIGHT\n        quantized_weight4bit = pack_weights(quantized_weight) \n\n        self.weight = quantized_weight4bit    \n        self.scale = scale\n    def forward(self, input):\n        # unpack the self.weight first\n        unpacked_weight = unpack_weights(self.weight)     \n        print(unpacked_weight.shape)\n        output = F.linear(input, unpacked_weight.to(input.dtype)) * self.scale\n        if self.bias is not None:\n            output = output + self.bias\n        return output\n    \ndef replace_linearlayer(base_model, quantizer_class, exception_list, quantized=True):\n    for name, child in base_model.named_children():\n        if isinstance(child, nn.Linear) and not any([x == name for x in exception_list]):\n            old_bias = child.bias\n            old_weight = child.weight\n            in_features = child.in_features\n            out_features = child.out_features\n\n          # intantiate a quantizer class layer\n            if quantized:\n                quantizer_layer = quantizer_class(in_features, out_features, old_bias is not None, old_weight.dtype).to(device)\n            else:\n                in_features = in_features//2\n                quantizer_layer = quantizer_class(in_features, out_features, old_bias is not None, old_weight.dtype).to(device)\n\n          # replace the name with quantizer_module layer\n            setattr(base_model, name, quantizer_layer)\n\n          # since the base_model name is now replaced with quantizer_module, we can call its quantize function to quantize the old_weight. the weight of the quantizer layer is a quantized weight with int8 type\n            if quantized:\n                getattr(base_model, name).quantize(old_weight)\n\n          # we can also update the quantizer module bias with the old_bias if it is not none\n            if old_bias is not None:\n                getattr(base_model, name).bias = old_bias\n\n        # if the child has further any sub linear layer, we can invoke the function again and loop inside the child. Pass the child in place of base_model\n        else:\n            replace_linearlayer(child, quantizer_class, exception_list, quantized=quantized)\n          ","metadata":{"execution":{"iopub.status.busy":"2024-06-29T09:21:16.722827Z","iopub.execute_input":"2024-06-29T09:21:16.724036Z","iopub.status.idle":"2024-06-29T09:21:16.744154Z","shell.execute_reply.started":"2024-06-29T09:21:16.723998Z","shell.execute_reply":"2024-06-29T09:21:16.741447Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def pack_weights(quantized_weight8bit):\n  # given a tensor with 2bit encoded value encoded_weight4bit. total number of value in encoded_weight4bit * bits(2bit encoded in this case). that should be divisible by 8. \n  # why divisible by 8? becuase we're storing number of 2bit encoded value in new 8bit tensor which sholud fit. PyTorch only support int8 precisio, not int2 or 4.     \n    bits = 4\n    if quantized_weight8bit.shape[-1] * bits % 8 != 0:\n        raise ValueError(\"encoded_weight4bit.shape[0] * bits shoul be divisible by 8\")\n\n  # total number of int8 values after int2 are packed in  \n    num_values = quantized_weight8bit.shape[-1] * bits // 8\n\n  # total number of 2-bit value within a single int8 packed tensor. num_values is total number of int8 packed tensor.\n    num_steps = 8 // bits    #8 is the total number of bit in 8bit and dividing by bits can give the total number of individual 2-bit encoded value located/packed inside that single int8 value tensor  \n    packed_weights = torch.zeros((quantized_weight8bit.shape[0], num_values), dtype=torch.int8).to(device)\n    weight_index = 0\n    \n    for row in range(quantized_weight8bit.shape[0]):\n        weight_index = 0\n        for i in range(num_values):\n            for j in range(num_steps):\n                if j==0 and quantized_weight8bit[row,weight_index] < 0:   \n                    encoded_weight4bit_zero = 0 # First value of packed_tensor shouldn't be negative - for now can't find logic yet\n                    packed_weights[row, i] |= encoded_weight4bit_zero << bits * j        \n                else:\n                    packed_weights[row, i] |= quantized_weight8bit[row,weight_index] << bits * j\n                weight_index += 1\n    return packed_weights\n  \ndef unpack_weights(packed_weights):\n  # how many 2-bit value are there in the entire packed_tensor\n  # first calcualte totals bits = packed_tensor.shape[0](total no of values in the packed tensor ) * 8 (each value is a unsigned 8bit tensor)\n  # then divide it by unpacked bits or original bits that we encoded - bits  \n    bits = 4\n    packed_weights = packed_weights.to(torch.int8).to(device)\n    num_values = packed_weights.shape[-1] * 8 // bits\n\n  # number of steps is how many encoded bits value is in the single packed tensor value\n    num_steps = 8 // bits\n\n  #lets initialized a unpacked_tensor with zero and later to be update with actual 2bit encoded value\n  # first we'll just extract encoded value in int8 and later we'll extract only the 2-bit part, we'll see how\n    unpacked_weights = torch.zeros((packed_weights.shape[0],num_values), dtype = torch.int8).to(device)\n    \n    for row in range(packed_weights.shape[0]):\n        unpacked_index = 0\n        for i in range(packed_weights.shape[-1]):\n            for j in range(num_steps):\n                unpacked_weights[row, unpacked_index] |= packed_weights[row, i] >> bits * j\n                unpacked_index += 1\n            mask = 2**bits - 1\n            unpacked_weights[row] &= mask\n            unpacked_weights[row] = optimize_unpacked_weights(unpacked_weights[row])\n    return unpacked_weights\n\ndef optimize_unpacked_weights(unpacked_weights):\n    updated_unpacked_weights = torch.zeros(unpacked_weights.shape[0], dtype=torch.int8).to(device)\n    for i in range(unpacked_weights.shape[0]):\n        a_binary = format(unpacked_weights[i].item(), '04b')\n        a3=int(a_binary[0])\n        a2=int(a_binary[1])\n        a1=int(a_binary[2])\n        a0=int(a_binary[3])\n\n        if a3 == 1:\n            updated_unpacked_weights[i] = -a3*pow(2, 3) + a2*pow(2, 2) + a1*pow(2, 1) + a0*pow(2, 0)\n        else:\n            updated_unpacked_weights[i]= a3*pow(2, 3) + a2*pow(2, 2) + a1*pow(2, 1) + a0*pow(2, 0)\n    return updated_unpacked_weights","metadata":{"execution":{"iopub.status.busy":"2024-06-29T09:21:22.343106Z","iopub.execute_input":"2024-06-29T09:21:22.344113Z","iopub.status.idle":"2024-06-29T09:21:22.361669Z","shell.execute_reply.started":"2024-06-29T09:21:22.344081Z","shell.execute_reply":"2024-06-29T09:21:22.360485Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", torch_dtype=torch.bfloat16)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:29:15.602290Z","iopub.execute_input":"2024-06-29T05:29:15.602699Z","iopub.status.idle":"2024-06-29T05:29:20.052071Z","shell.execute_reply.started":"2024-06-29T05:29:15.602657Z","shell.execute_reply":"2024-06-29T05:29:20.051062Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"447f1cc4c5524f9895aea30fa3c7a77a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a844685d5274476b23e626011bcbaea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55ad38c77f744606ad4ae61380821d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"462943e38cd54574b82d98cf97de8c17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d10f2c9568d4c5d9184ffb014a6f8d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7937bfac14984aa5aea601319ccd5a54"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6296c566baf94bf484d04da0ed52bbc3"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"OPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0-11): 12 x OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"print(\"facebook/opt-125m: base model architecture\")\nprint(\"-\"*50)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:29:23.726776Z","iopub.execute_input":"2024-06-29T05:29:23.727649Z","iopub.status.idle":"2024-06-29T05:29:23.734237Z","shell.execute_reply.started":"2024-06-29T05:29:23.727614Z","shell.execute_reply":"2024-06-29T05:29:23.732752Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"facebook/opt-125m: base model architecture\n--------------------------------------------------\nOPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0-11): 12 x OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_memory_size_before_quantization = model.get_memory_footprint()\nprint(f\"Total memory size before quantization (in GB): {model_memory_size_before_quantization / 1e+9}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:29:28.002897Z","iopub.execute_input":"2024-06-29T05:29:28.003291Z","iopub.status.idle":"2024-06-29T05:29:28.010576Z","shell.execute_reply.started":"2024-06-29T05:29:28.003260Z","shell.execute_reply":"2024-06-29T05:29:28.009436Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Total memory size before quantization (in GB): 0.250478592\n","output_type":"stream"}]},{"cell_type":"code","source":"# Let's perform inference on this facebook/opt-125m base model\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\npipe(\"Malaysia is a beautiful country and \", max_new_tokens=50)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:29:31.427294Z","iopub.execute_input":"2024-06-29T05:29:31.428162Z","iopub.status.idle":"2024-06-29T05:29:32.936579Z","shell.execute_reply.started":"2024-06-29T05:29:31.428127Z","shell.execute_reply":"2024-06-29T05:29:32.935549Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': \"Malaysia is a beautiful country and  I'm glad to see it is getting better.\\nI'm glad to see Malaysia getting better.  I'm glad to see Malaysia getting better.  I'm glad to see Malaysia getting better.  I'm glad to see Malaysia getting better.\"}]"},"metadata":{}}]},{"cell_type":"code","source":"# The base model seems to be working good. Lets start quantizing this facebook/opt-125m model by calling our custom quantizer which we've builded earlier.\nreplace_linearlayer(model, QuantizedLinearLayer, [\"lm_head\"], quantized=True)\nprint(\"facebook/opt-125m: quantized model architecture\")\nprint(\"-\"*50)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T05:29:40.723170Z","iopub.execute_input":"2024-06-29T05:29:40.723551Z","iopub.status.idle":"2024-06-29T06:47:02.632810Z","shell.execute_reply.started":"2024-06-29T05:29:40.723519Z","shell.execute_reply":"2024-06-29T06:47:02.631734Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"facebook/opt-125m: quantized model architecture\n--------------------------------------------------\nOPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0-11): 12 x OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): QuantizedLinearLayer()\n            (v_proj): QuantizedLinearLayer()\n            (q_proj): QuantizedLinearLayer()\n            (out_proj): QuantizedLinearLayer()\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): QuantizedLinearLayer()\n          (fc2): QuantizedLinearLayer()\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_memory_size_after_quantization = model.get_memory_footprint()\nprint(f\"Total memory size after quantization (in GB): {model_memory_size_after_quantization / 1e+9}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:48:11.097072Z","iopub.execute_input":"2024-06-29T06:48:11.097481Z","iopub.status.idle":"2024-06-29T06:48:11.105233Z","shell.execute_reply.started":"2024-06-29T06:48:11.097451Z","shell.execute_reply":"2024-06-29T06:48:11.104014Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Total memory size after quantization (in GB): 0.123242496\n","output_type":"stream"}]},{"cell_type":"code","source":"# save the quantize model and push to huggingface hub for later inferencing use for all users\n# saving in local directory\ntorch.save(model.state_dict(), \"quantized_opt125_state_dict.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:48:44.488563Z","iopub.execute_input":"2024-06-29T06:48:44.489682Z","iopub.status.idle":"2024-06-29T06:48:44.739064Z","shell.execute_reply.started":"2024-06-29T06:48:44.489628Z","shell.execute_reply":"2024-06-29T06:48:44.737954Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"api = HfApi()\n\napi.upload_file(\n    path_or_fileobj=\"quantized_opt125_state_dict.pth\",\n    path_in_repo = \"quantized_opt125_state_dict.pth\",\n    repo_id = \"tamangmilan/quantized_facebook_opt_125m\",    \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T06:49:24.043352Z","iopub.execute_input":"2024-06-29T06:49:24.044382Z","iopub.status.idle":"2024-06-29T06:49:30.635966Z","shell.execute_reply.started":"2024-06-29T06:49:24.044336Z","shell.execute_reply":"2024-06-29T06:49:30.634997Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"quantized_opt125_state_dict.pth:   0%|          | 0.00/123M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e373f52836944978bfeb5441bf5fa01"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/tamangmilan/quantized_facebook_opt_125m/commit/9698c0f8724a93d04f46db33df3eb27390f8ac57', commit_message='Upload quantized_opt125_state_dict.pth with huggingface_hub', commit_description='', oid='9698c0f8724a93d04f46db33df3eb27390f8ac57', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# AutoConfig gets all the skeleton of model architecture \nconfig = AutoConfig.from_pretrained(\"facebook/opt-125m\")\n\n# using this skeleton, we'll initialize the model withouht weight, just empty model with everythign else same of the architecture\nwith torch.device(\"meta\"):\n  new_model = OPTForCausalLM(config)\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\nprint(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T09:21:34.522946Z","iopub.execute_input":"2024-06-29T09:21:34.523757Z","iopub.status.idle":"2024-06-29T09:21:35.875572Z","shell.execute_reply.started":"2024-06-29T09:21:34.523726Z","shell.execute_reply":"2024-06-29T09:21:35.874510Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d59eddf1809242e19400d75c978609f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"550e7c7e48914bffadcaf388a54c8a86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"052ef0e800774e0cafffe11cda56054e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc41f0c796a3449eaa8a31290432fe08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c075eb6135a4c398cfc202243ba1a31"}},"metadata":{}},{"name":"stdout","text":"OPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0-11): 12 x OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Replace only the linear layer but dont perform quantization,hence quantized=False.\nreplace_linearlayer(new_model, QuantizedLinearLayer, [\"lm_head\"], quantized=False)\nprint(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T09:21:44.534669Z","iopub.execute_input":"2024-06-29T09:21:44.535672Z","iopub.status.idle":"2024-06-29T09:21:45.537870Z","shell.execute_reply.started":"2024-06-29T09:21:44.535639Z","shell.execute_reply":"2024-06-29T09:21:45.537004Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"OPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0-11): 12 x OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): QuantizedLinearLayer()\n            (v_proj): QuantizedLinearLayer()\n            (q_proj): QuantizedLinearLayer()\n            (out_proj): QuantizedLinearLayer()\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): QuantizedLinearLayer()\n          (fc2): QuantizedLinearLayer()\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"state_dict_cache_path = hf_hub_download(\n    repo_id=\"tamangmilan/quantized_facebook_opt_125m\",    \n    filename=\"quantized_opt125_state_dict.pth\"\n)\nstate_dict = torch.load(state_dict_cache_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T09:21:53.321239Z","iopub.execute_input":"2024-06-29T09:21:53.321626Z","iopub.status.idle":"2024-06-29T09:21:54.546272Z","shell.execute_reply.started":"2024-06-29T09:21:53.321598Z","shell.execute_reply":"2024-06-29T09:21:54.545400Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"quantized_opt125_state_dict.pth:   0%|          | 0.00/123M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"218ddcf51596434bb717b9d967c5e3c1"}},"metadata":{}}]},{"cell_type":"code","source":"new_model.load_state_dict(state_dict, strict=True, assign=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T09:21:58.656982Z","iopub.execute_input":"2024-06-29T09:21:58.657344Z","iopub.status.idle":"2024-06-29T09:21:58.676498Z","shell.execute_reply.started":"2024-06-29T09:21:58.657315Z","shell.execute_reply":"2024-06-29T09:21:58.675608Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Let's perform inference on this gemma-2b base model\npipe = pipeline(\"text-generation\", model=new_model, tokenizer=tokenizer)\npipe(\"Malaysia is a beautiful country and \", max_new_tokens=50)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T09:22:21.737704Z","iopub.execute_input":"2024-06-29T09:22:21.738435Z","iopub.status.idle":"2024-06-29T13:41:22.161592Z","shell.execute_reply.started":"2024-06-29T09:22:21.738404Z","shell.execute_reply":"2024-06-29T13:41:22.160422Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"torch.Size([768, 768])\ntorch.Size([768, 768])\n","output_type":"stream"},{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]}]}